{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#特征工程\" data-toc-modified-id=\"特征工程-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>特征工程</a></span><ul class=\"toc-item\"><li><span><a href=\"#为什么需要对数值型特征进行归一化处理？\" data-toc-modified-id=\"为什么需要对数值型特征进行归一化处理？-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>为什么需要对数值型特征进行归一化处理？</a></span></li><li><span><a href=\"#常用的归一化方法有哪些？\" data-toc-modified-id=\"常用的归一化方法有哪些？-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>常用的归一化方法有哪些？</a></span></li><li><span><a href=\"#是否所有模型都要求进行特征归一化？\" data-toc-modified-id=\"是否所有模型都要求进行特征归一化？-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>是否所有模型都要求进行特征归一化？</a></span></li><li><span><a href=\"#类别型特征进行编码？\" data-toc-modified-id=\"类别型特征进行编码？-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>类别型特征进行编码？</a></span></li><li><span><a href=\"#什么是特征组合？\" data-toc-modified-id=\"什么是特征组合？-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>什么是特征组合？</a></span></li><li><span><a href=\"#如何处理高维组合特征？\" data-toc-modified-id=\"如何处理高维组合特征？-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>如何处理高维组合特征？</a></span></li><li><span><a href=\"#文本特征表示\" data-toc-modified-id=\"文本特征表示-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>文本特征表示</a></span></li></ul></li><li><span><a href=\"#评估准则\" data-toc-modified-id=\"评估准则-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>评估准则</a></span><ul class=\"toc-item\"><li><span><a href=\"#准确率\" data-toc-modified-id=\"准确率-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>准确率</a></span></li><li><span><a href=\"#精确率/召回率/F-Score\" data-toc-modified-id=\"精确率/召回率/F-Score-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>精确率/召回率/F-Score</a></span></li><li><span><a href=\"#ROC曲线-vs-ROC曲线\" data-toc-modified-id=\"ROC曲线-vs-ROC曲线-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>ROC曲线 vs ROC曲线</a></span></li><li><span><a href=\"#余弦距离及其应用\" data-toc-modified-id=\"余弦距离及其应用-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>余弦距离及其应用</a></span></li><li><span><a href=\"#不平衡数据如何选择评估准则\" data-toc-modified-id=\"不平衡数据如何选择评估准则-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>不平衡数据如何选择评估准则</a></span></li></ul></li><li><span><a href=\"#模型评估方法\" data-toc-modified-id=\"模型评估方法-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>模型评估方法</a></span></li><li><span><a href=\"#线性回归\" data-toc-modified-id=\"线性回归-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>线性回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型原理\" data-toc-modified-id=\"模型原理-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>模型原理</a></span></li><li><span><a href=\"#参数推导：-矩阵直接求解\" data-toc-modified-id=\"参数推导：-矩阵直接求解-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>参数推导： 矩阵直接求解</a></span></li><li><span><a href=\"#参数推导：梯度下降\" data-toc-modified-id=\"参数推导：梯度下降-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>参数推导：梯度下降</a></span></li></ul></li><li><span><a href=\"#正则化\" data-toc-modified-id=\"正则化-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>正则化</a></span><ul class=\"toc-item\"><li><span><a href=\"#l1正则化和l2正则化\" data-toc-modified-id=\"l1正则化和l2正则化-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>l1正则化和l2正则化</a></span></li><li><span><a href=\"#从概率的角度解释l1正则化和l2正则化\" data-toc-modified-id=\"从概率的角度解释l1正则化和l2正则化-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>从概率的角度解释l1正则化和l2正则化</a></span></li></ul></li><li><span><a href=\"#逻辑回归\" data-toc-modified-id=\"逻辑回归-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>逻辑回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型原理\" data-toc-modified-id=\"模型原理-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>模型原理</a></span></li><li><span><a href=\"#最大似然和对数损失\" data-toc-modified-id=\"最大似然和对数损失-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>最大似然和对数损失</a></span></li><li><span><a href=\"#多分类和softmax\" data-toc-modified-id=\"多分类和softmax-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>多分类和softmax</a></span></li></ul></li><li><span><a href=\"#广义线性模型\" data-toc-modified-id=\"广义线性模型-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>广义线性模型</a></span></li><li><span><a href=\"#决策树模型\" data-toc-modified-id=\"决策树模型-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>决策树模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#ID3\" data-toc-modified-id=\"ID3-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>ID3</a></span></li><li><span><a href=\"#C4.5\" data-toc-modified-id=\"C4.5-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>C4.5</a></span></li><li><span><a href=\"#CART\" data-toc-modified-id=\"CART-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>CART</a></span></li><li><span><a href=\"#剪枝方法\" data-toc-modified-id=\"剪枝方法-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>剪枝方法</a></span></li><li><span><a href=\"#连续值的处理\" data-toc-modified-id=\"连续值的处理-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>连续值的处理</a></span></li><li><span><a href=\"#缺失值处理\" data-toc-modified-id=\"缺失值处理-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>缺失值处理</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>PCA</a></span></li><li><span><a href=\"#Bagging集成\" data-toc-modified-id=\"Bagging集成-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Bagging集成</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#Booting集成\" data-toc-modified-id=\"Booting集成-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Booting集成</a></span><ul class=\"toc-item\"><li><span><a href=\"#AdaBoosting\" data-toc-modified-id=\"AdaBoosting-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>AdaBoosting</a></span></li><li><span><a href=\"#GBDT\" data-toc-modified-id=\"GBDT-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>GBDT</a></span></li><li><span><a href=\"#XGBOOST\" data-toc-modified-id=\"XGBOOST-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>XGBOOST</a></span></li></ul></li><li><span><a href=\"#CNN模型\" data-toc-modified-id=\"CNN模型-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>CNN模型</a></span></li><li><span><a href=\"#RNN模型\" data-toc-modified-id=\"RNN模型-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>RNN模型</a></span></li><li><span><a href=\"#LSTM模型\" data-toc-modified-id=\"LSTM模型-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>LSTM模型</a></span></li><li><span><a href=\"#GRU模型\" data-toc-modified-id=\"GRU模型-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>GRU模型</a></span></li><li><span><a href=\"#主题模型\" data-toc-modified-id=\"主题模型-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>主题模型</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要对数值型特征进行归一化处理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了消除不同特征之间量纲的影响，将特征都统一到相同的数值区间中。并且进行归一化处理可以更快找到最优的参数，模型在数值区间范围更广的特征上需要更多次的迭代找到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用的归一化方法有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用的方法有Min-Max Scaling以及Z-Score。\n",
    "\n",
    "Min-Max Scaling $$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "该方法将数据归一化到$[0,1]$的范围中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Score $$x_{norm} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "该方法将数据归一化到标准正态分布上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 是否所有模型都要求进行特征归一化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不是，通常来说使用梯度下降算法的模型需要归一化。比如，线性回归，逻辑回归，神经网络模型等。但是对于一些模型则不需要，比如决策树模型。这是因为决策树模型在选择最优属性时使用的信息增益（ID3）、信息增益率（C4.5）或者基尼指数都与数据的范围无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别型特征进行编码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 序号编码：通常用于处理类别之间有大小关系的特征。\n",
    "+ onehot编码：适用于处理类别之间没有大小关系的特征。但是当类别取值的个数太多时将带来高维度问题，将需要大量的空间存储数据。此时可以考虑使用稀疏向量存储特征，或者考虑使用二进制编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是特征组合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将两个一维离散特征组合成一个特征，从而可以挖掘多个特征共同对目标的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何处理高维组合特征？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本特征表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ one-hot encoding: 当单词数量多的时候，特征的维度会特别大。特别的，对于短文本往往还会有稀疏问题。\n",
    "+ term-frequency: $\\frac{词语在文章出现的次数}{文章中的总词语数量}$。 TF的一个问题就是对于一些常用的词语，比如“我”，“的”，“是”等在文章中出现的次数比较多，所以TF的值也相对高，但是这些词语并没有很重要的意义，因为它们都是常见的词语无法表达文章的特别性，因此有了TF-IDF。\n",
    "+ TF-IDF: $$tf * idf$$ $$idf(w) = \\frac{总文章数}{包含词语w的文章数+1}$$\n",
    "idf可以表示词语在整个数据集中的重要性，如果包含词语w的文章数多，那么其idf就小。\n",
    "+ 词嵌入：将词语表示成一个n维实数向量，如果两个向量距离近则表示它们对应的词语的语义相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$accuracy = \\frac{正确分类的样本}{总样本数}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率计算简单，但是并不适用于类别不平衡的数据。因为，假设模型将所有样本直接预测为类别多的样本，那么准确率也会很高，但是实际上这个模型确是很差的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精确率/召回率/F-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$F1-Score = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n",
    "$$F_{\\beta}-Score = \\frac{(1+\\beta^2) * Precision * Recall}{\\beta^2 * Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 精确率表示模型预测为正类的样本中，有多少是正类的。但是这个指标并不适用于类别不平衡的数据，假设模型直接预测每个样本的类别为数据量多的类别，那么精确度会很高，但是无法反映模型的好坏。\n",
    "+ 召回率表示确实为正类的样本有多少被正确预测。但是召回率高也并不一定是好的，考虑检索场景下，假设返回了尽可能多的结，这样召回率高但是可能返回的结果与搜索关键词的相关度都并不是很高，从而导致精确率很低。所以往往需要综合考虑这两个指标。\n",
    "+ F1-Score是一个等价综合考虑精确率和召回率的指标。$F_\\beta-Score$则可以通过调节$\\beta$的大小来调整召回率相对精确率的重要性，当$\\beta$大于1时则召回率更重要，反之当$\\beta$小于1时则精确率更重要。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC曲线 vs ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 余弦距离及其应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不平衡数据如何选择评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 使用ROC曲线而不是PR曲线作为评估准则，因为ROC曲线不会受到类别不平衡的影响。\n",
    "+ 使用Precison/Recall/F-Score作为综合评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数推导： 矩阵直接求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数推导：梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1正则化和l2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从概率的角度解释l1正则化和l2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然和对数损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类和softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续值的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booting集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
