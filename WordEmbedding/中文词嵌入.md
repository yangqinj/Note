# 中文词嵌入

当前词嵌入(word embedding)模型绝大多数是基于英文的，专门面向中文的词嵌入模型则非常少。本篇博客整理了当前的中文词嵌入学习模型，包括CWE (Chen et al. 2015)、MGE (Yin et al. 2016)、JWE (Yu et al. 2017)、GWE (Su and Lee, 2017)和CW2VEC (Cao et al. 2017)等。除上述中文词嵌入模型之外，还有学者提出了中文字嵌入模型(Yanran Li et al. 2015)，搜狗公司提出的中文偏旁部首嵌入(Xinlei Shi et al. 2015)等。

### CWE
CWE (character-enhanced word embedding model) (Chen et al. 2015) 是目前我能找到的最早学习中文词嵌入的模型。由Chen等人与2015年提出，论文发表在IJCAI会议上。不同于英文，中文的词是由一个一个的字组成的，这些字包含了丰富的语义和语音信息。CWE基于该特点提出在CBOW基础上引入单词中的字作为上下文。模型的结构如下图所示：

![CWE](https://user-images.githubusercontent.com/9667328/36341280-c2200666-1426-11e8-90fc-e13b98fdca41.png)

CWE将每个词$x_j$表示为:

$$x_j = \frac{1}{2} (w_j + \frac{1}{N_j}\sum{k=1}^{N_j}c_k)$$

其中，$w_j$是$x_j$的词嵌入，$N_j$是组成词语的字数，$c_k$是每个字的嵌入表示。CWE使用负采样方法学习词嵌入。但是，单个字在不同的词语中可能表示不同的意思。为此，Chen等人提出了CWE的三种变形：基于位置的字嵌入(CWE+P)，基于簇的字嵌入(CWE+C)和非参数基于簇的字嵌入(CWE+NC)。CWE+P考虑了字在词语中的位置，因为不同的字在不同词语中的不同位置可能表示不同的语义。对于每个字*c*，CWE+P学习三种字嵌入$(c^B, c^M, c^E)$，分别对应了前中后三种位置。现在，每个词$x_j$表示为：

$$x_j = \frac{1}{2} (w_j + \frac{1}{N_j}(c_1^B + \sum{k=2}^{N_j-1}c_k^M + c_{N_j}^E))$$

![CWE+P](/Users/yangqj/Documents/Documents/知识就是力量/Note/WordEmbedding/36341550-ddb778ec-142a-11e8-9396-1a6521d3da35.png)

CWE+C则认为一个字的不同语义可以根据其对应的上下文识别，相似的上下文对应的字表达相似的语义。因此，CWE+C将字对应的上下文使用K-means方法聚类，属于不同簇的字对应不同的字嵌入。
现在，每个词$x_j$表示为：

$$x_j = \frac{1}{2} (w_j + \frac{1}{N_j}\sum{k=1}^{N_j}c_k^{r_k^{max}})$$

其中，$r_k^{max}$表示第*k*个字最可能属于哪一个簇。CWE+NC方法则在CWE+C的基础上使用在线非参数聚类方法学习。
![CWE+C](https://user-images.githubusercontent.com/9667328/36341556-f8a7c5a8-142a-11e8-8a6a-bbf80f35ce5e.png)

### MGE
MGE (multi-granularity embedding) (Yin et al. 2016)在CWE的基础上，补充字的偏旁部首作为上下文。MGE认为字的偏旁部首也可以表示字的语义或者语音信息。比如，“氵”作为“海”，“湖”，“河”等字的部首表示这些字跟水有关，“马”在“妈”，“码”，“骂”等字都出现了，与这些字有相同的发音。
![MGE](https://user-images.githubusercontent.com/9667328/36341674-a0353d5e-142c-11e8-9cb4-b4865f74cbf4.png)

MGE的目标为最大化似然函数：

$$L(D) = \sum_{w_i \in D} log p(w_i|h_i)$$

每个目标词语\\(w_i\\)的上下文\\(h_i\\)定义为：

$$h_i = \frac{1}{2} \big( \frac{1}{|W_i|} \sum_{w_j \in W_i} (w_j + \frac{1}{|C_j|}\sum_{c_k \in C_j}c_k) + \frac{1}{|R_i|} \sum_{r_k \in R_i}r_k \big)$$

注意，MGE中只使用了目标词语的部首。

### JWE
JWE (joint learning word embedding model) (Yu et al. 2017) 认为只考虑组成词语的字或者字的部首无法充分利用字丰富的结构信息，比如“照”字的语义需要通过其组成成分“灬”，“日”和“刀”和“口”共同体现。
![JWE](https://user-images.githubusercontent.com/9667328/36341762-3fcd0dc8-142e-11e8-8884-6facd7ce5a77.png)

JWE的目标为最大化三个似然函数的和：

$$L(w_i) = \sum_{k=1}^3 log p(w_i|h_{ik})$$

这个目标函数与MGE的不同之处在于，JWE将不同来源的嵌入（字，部首，组成成分）解耦了。当更新这些嵌入时，不同来源的嵌入更新的梯度不同。最终可以学习词嵌入、字嵌入、部首嵌入和组成成分嵌入。JWE实验也证明了对不同来源的嵌入解耦确实可以获得更好的结果。不同来源的上下文$h_{ik}$是其对应上下文向量的平均。

### GWE
GWE (Glyph-EnhancedWord Embedding) (Su and Lee, 2017) 期望通过字的笔画顺序来增强中文词嵌入学习。特别的是，GWE从图形学的角度，利用卷积自动编码器（convolutional auto-encoder，简称convAE）从字的位图中提取笔画特征。
![GWE](https://user-images.githubusercontent.com/9667328/36375975-68c6fb2e-15ac-11e8-8671-43d7962104b5.png)

类似于MGE，GWE将每个词的上下文\\(h_i\\)表示为：

$$h_i = \frac{1}{|W(i)|} \sum_{w_i \in W(i)} (w_i + \frac{1}{|C(i)|}\sum_{c_j \in C(i)}\hat{c_j} + \hat{g_j})$$

其中$hat{g_j}$表示字的笔画特征。

### CW2VEC
CW2VEC (Cao et al. 2017) 也是期望使用字的笔画特征来提高中文词嵌入学习，不过CW2VEC采用的是笔画n元(stroke n-gram)特征。CW2VEC给所有的中文笔画编号后，将每个字分解为笔画n元编号，具体的过程如下图的例子所示：
![Example](https://user-images.githubusercontent.com/9667328/36376042-ae4fa39e-15ac-11e8-89c5-23c97f093d31.png)

不同于上述中文词嵌入学习模型，CW2VEC不是基于CBOW而是Skip-gram。并且，CW2VEC重新定义了衡量两个词语相似度的方法：

$$sim(w, c) = \sum_{q \in S(w)} \hat{q} \cdot \hat{c}$$

其中，$S(w)$是词语*w*的笔画n元集，$\hat{q}$表示某个笔画元*q*的嵌入表示，$\hat{c}$则是词语*c*的词嵌入。

> 之所以需要重新定义相似度衡量方法，是因为在CW2VEC中词语是从笔画级别表示的，而不是之前从词语的级别。不过我迷惑的是为什么不定义相似度衡量方法为两个词语笔画n元嵌入平均值的点积。我猜测是因为这样会消减笔画n元特征的作用，将完全不同的笔画n元统一到了一个*N*维的实数向量中。又或者相似度衡量方法为什么不定义为两个词语各个笔画n元点积之和？这样不会消减笔画n元特征，也可以从相同的笔画级别衡量相似度？

### 实验对比
##### 数据集和模型参数设置
为了对比上述模型，我们下载了[中文维基百科语料库](https://dumps.wikimedia.org/zhwiki/)，并使用[Wikipedia Extractor](http://medialab.di.unipi.it/wiki/Wikipedia_Extractor)提取正文，[opencc](https://github.com/BYVoid/OpenCC)将文本统一为简体中文。随后，保留了编码在0x4E00和0x9FA5之间的中文字符，使用[thulac](https://github.com/thunlp/THULAC)工具完成分词和词性标注。

词语相似度是衡量词嵌入质量最常用的方法，其旨在衡量词嵌入捕获词语语义相似度的能力。当前最常用的中文词语相似度数据集是wordsim-240和wordsim-296（Jin and Wu, 2012），可以从[这里](https://github.com/Leonard-Xu/CWE/tree/master/data)下载。该任务使用斯皮尔曼相关系数表示模型的好坏。实验的结果如下：

| Model | Wordsim-240 | Wordsim-297 |
| ----- | ----------- | ----------- |
| CBOW  | 0.5322      | 0.5746      |
| CWE   | 0.5138      | 0.6022      |
| MGE   | 0.4635      | 0.5231      |
| JWE   | **0.5706**  | **0.6541**  |

#### 词语推断
词语推断测试的是词嵌入推断不同词语对之间语义相关度的能力。给定三个词语*a, b, c*，任务的目标是找到第四个词语*d*使得这些词语之间存在关系：*a*对于*b*就像*c*对于*d*。常用的中文词语推断数据集是[(Chen et al. 2015提供的)](https://github.com/Leonard-Xu/CWE/tree/master/data)。该数据集中的词语对来自三个类别，即国家首都、省会和家庭。
词语推断的方法有两种*3CosAdd* (Mikolov et al. 2013) 和*3CosMul* (Levy and Goldberg, 2014)。前者相当于一个线性组合，其忽略了不同方面的差异且这些差异有不同的强度，从而使得较大的差异占优势；后者则可以放大词嵌入之间的微小差异，同时减小词嵌入之间的较大差异。实验的结果如下表所示：

  **3CosADD**

| Model | Total | Capital | City | Family |
| ----- | ----- | ------- | ---- | ------ |
| CBOW | 0.6699 | 0.7622 | 0.7200 | 0.4081 |
| CWE | 0.7687 | 0.8744 | 0.88 | 0.4338 |
| MGE | 0.6388 | 0.7696 | 0.8343 | 0.1875 |
| JWE | **0.8301** | **0.8953** | **0.9200** | **0.6103** |


 **3CosMul**

| Model | Total | Capital | City | Family |
| ----- | ----- | ------- | ---- | ------ |
| CBOW | 0.7536 | 0.7563 | 0.6971 | 0.3971 |
| CWE | 0.7687 | 0.8538 | 0.8629 | 0.4338 |
| MGE | 0.7415 | 0.8171 | 0.1654 | **0.6139** |
| JWE | **0.8185** | **0.8847** | **0.9086** | 0.5956 |


> 词嵌入质量的衡量方法除了词语相似度和词语推断，也可以从一些NLP任务入手，比如文本分类、中文分词、命名实体识别等等。此外，案例分析也是常用的方法。不过案例分析并不全面，大多数取决于选择的词语，但也是一种定性分析方法，可以观察出一些结论。

### 参考
(Chen et al. 2015) X. Chen, L. Xu, Z. Liu, M. Sun, and H. Luan, “Joint learning of character and word embeddings,” in Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015, pp. 1236–1242.

(Yin et al. 2016) R. Yin, Q. Wang, P. Li, R. Li, and B. Wang, “Multi-granularity chinese word embedding,” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016, pp. 981–986.

(Yu et al. 2017) J. Yu, X. Jian, H. Xin, and Y. Song, “Joint embeddings of chinese words, characters, and fine-grained subcharacter components,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 286–291.

(Su and Lee, 2017) T. Su and H. Lee, “Learning chinese word representations from glyphs of characters,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 264–273.

(Cao et al. 2017) Cao Shaosheng, Lu Wei, Zhou Jun and Li Xiaolong, “cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information,” in Proceedings of the 32th AAAI Conference on Artificial Intelligence, 2017.

(Mikolov et al. 2013) T. Mikolov, W. Yih, and G. Zweig, “Linguistic regularities in continuous space word representations,” in Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, 2013, pp. 746–751.

(Levy and Goldberg, 2014) O. Levy and Y. Goldberg, “Linguistic regularities in sparse and explicit word representations,” in Proceedings of the Eighteenth Conference on Computational Natural Language Learning, 2014, pp. 171–180.

(Jin and Wu, 2012) Jin, P., and Wu, Y. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In SemEval.

(Li et al. 2015) Y. Li, W. Li, F. Sun, and S. Li, “Component-Enhanced Chinese Character Embeddings,” in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, no. September, pp. 829–834.
(Shi et al. 2015) X. Shi, J. Zhai, X. Yang, Z. Xie, and C. Liu, “Radical Embedding : Delving Deeper to Chinese Radicals,” in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), 2015, pp. 594–598.
