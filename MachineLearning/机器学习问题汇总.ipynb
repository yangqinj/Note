{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#特征工程\" data-toc-modified-id=\"特征工程-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>特征工程</a></span><ul class=\"toc-item\"><li><span><a href=\"#为什么需要对数值型特征进行归一化处理？\" data-toc-modified-id=\"为什么需要对数值型特征进行归一化处理？-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>为什么需要对数值型特征进行归一化处理？</a></span></li><li><span><a href=\"#常用的归一化方法有哪些？它们的缺点是什么？\" data-toc-modified-id=\"常用的归一化方法有哪些？它们的缺点是什么？-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>常用的归一化方法有哪些？它们的缺点是什么？</a></span></li><li><span><a href=\"#是否所有模型都要求进行特征归一化？\" data-toc-modified-id=\"是否所有模型都要求进行特征归一化？-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>是否所有模型都要求进行特征归一化？</a></span></li><li><span><a href=\"#类别型特征进行编码？\" data-toc-modified-id=\"类别型特征进行编码？-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>类别型特征进行编码？</a></span></li><li><span><a href=\"#文本特征表示\" data-toc-modified-id=\"文本特征表示-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>文本特征表示</a></span></li><li><span><a href=\"#学习词嵌入的模型有哪些？\" data-toc-modified-id=\"学习词嵌入的模型有哪些？-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>学习词嵌入的模型有哪些？</a></span></li><li><span><a href=\"#如何评估词嵌入的好坏\" data-toc-modified-id=\"如何评估词嵌入的好坏-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>如何评估词嵌入的好坏</a></span></li><li><span><a href=\"#word2vec模型\" data-toc-modified-id=\"word2vec模型-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>word2vec模型</a></span></li><li><span><a href=\"#Glove模型\" data-toc-modified-id=\"Glove模型-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Glove模型</a></span></li><li><span><a href=\"#fasttext模型\" data-toc-modified-id=\"fasttext模型-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>fasttext模型</a></span></li><li><span><a href=\"#ELMo模型\" data-toc-modified-id=\"ELMo模型-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>ELMo模型</a></span></li><li><span><a href=\"#BERT模型\" data-toc-modified-id=\"BERT模型-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>BERT模型</a></span></li></ul></li><li><span><a href=\"#评估准则\" data-toc-modified-id=\"评估准则-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>评估准则</a></span><ul class=\"toc-item\"><li><span><a href=\"#准确率\" data-toc-modified-id=\"准确率-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>准确率</a></span></li><li><span><a href=\"#精确率/召回率/F-Score\" data-toc-modified-id=\"精确率/召回率/F-Score-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>精确率/召回率/F-Score</a></span></li><li><span><a href=\"#ROC曲线-vs-ROC曲线\" data-toc-modified-id=\"ROC曲线-vs-ROC曲线-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>ROC曲线 vs ROC曲线</a></span></li><li><span><a href=\"#余弦距离及其应用\" data-toc-modified-id=\"余弦距离及其应用-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>余弦距离及其应用</a></span></li><li><span><a href=\"#不平衡数据如何选择评估准则\" data-toc-modified-id=\"不平衡数据如何选择评估准则-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>不平衡数据如何选择评估准则</a></span></li></ul></li><li><span><a href=\"#模型评估方法\" data-toc-modified-id=\"模型评估方法-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>模型评估方法</a></span></li><li><span><a href=\"#线性回归\" data-toc-modified-id=\"线性回归-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>线性回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型原理\" data-toc-modified-id=\"模型原理-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>模型原理</a></span></li><li><span><a href=\"#参数推导：-矩阵直接求解\" data-toc-modified-id=\"参数推导：-矩阵直接求解-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>参数推导： 矩阵直接求解</a></span></li><li><span><a href=\"#参数推导：梯度下降\" data-toc-modified-id=\"参数推导：梯度下降-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>参数推导：梯度下降</a></span></li></ul></li><li><span><a href=\"#正则化\" data-toc-modified-id=\"正则化-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>正则化</a></span><ul class=\"toc-item\"><li><span><a href=\"#l1正则化和l2正则化\" data-toc-modified-id=\"l1正则化和l2正则化-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>l1正则化和l2正则化</a></span></li><li><span><a href=\"#从概率的角度解释l1正则化和l2正则化\" data-toc-modified-id=\"从概率的角度解释l1正则化和l2正则化-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>从概率的角度解释l1正则化和l2正则化</a></span></li></ul></li><li><span><a href=\"#逻辑回归\" data-toc-modified-id=\"逻辑回归-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>逻辑回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型原理\" data-toc-modified-id=\"模型原理-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>模型原理</a></span></li><li><span><a href=\"#最大似然和对数损失\" data-toc-modified-id=\"最大似然和对数损失-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>最大似然和对数损失</a></span></li><li><span><a href=\"#多分类和softmax\" data-toc-modified-id=\"多分类和softmax-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>多分类和softmax</a></span></li></ul></li><li><span><a href=\"#广义线性模型\" data-toc-modified-id=\"广义线性模型-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>广义线性模型</a></span></li><li><span><a href=\"#决策树模型\" data-toc-modified-id=\"决策树模型-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>决策树模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#ID3\" data-toc-modified-id=\"ID3-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>ID3</a></span></li><li><span><a href=\"#C4.5\" data-toc-modified-id=\"C4.5-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>C4.5</a></span></li><li><span><a href=\"#CART\" data-toc-modified-id=\"CART-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>CART</a></span></li><li><span><a href=\"#剪枝方法\" data-toc-modified-id=\"剪枝方法-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>剪枝方法</a></span></li><li><span><a href=\"#连续值的处理\" data-toc-modified-id=\"连续值的处理-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>连续值的处理</a></span></li><li><span><a href=\"#缺失值处理\" data-toc-modified-id=\"缺失值处理-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>缺失值处理</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>PCA</a></span></li><li><span><a href=\"#Bagging集成\" data-toc-modified-id=\"Bagging集成-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Bagging集成</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#Booting集成\" data-toc-modified-id=\"Booting集成-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Booting集成</a></span><ul class=\"toc-item\"><li><span><a href=\"#AdaBoosting\" data-toc-modified-id=\"AdaBoosting-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>AdaBoosting</a></span></li><li><span><a href=\"#GBDT\" data-toc-modified-id=\"GBDT-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>GBDT</a></span></li><li><span><a href=\"#XGBOOST\" data-toc-modified-id=\"XGBOOST-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>XGBOOST</a></span></li></ul></li><li><span><a href=\"#CNN模型\" data-toc-modified-id=\"CNN模型-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>CNN模型</a></span></li><li><span><a href=\"#RNN模型\" data-toc-modified-id=\"RNN模型-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>RNN模型</a></span></li><li><span><a href=\"#LSTM模型\" data-toc-modified-id=\"LSTM模型-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>LSTM模型</a></span></li><li><span><a href=\"#GRU模型\" data-toc-modified-id=\"GRU模型-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>GRU模型</a></span></li><li><span><a href=\"#主题模型\" data-toc-modified-id=\"主题模型-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>主题模型</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要对数值型特征进行归一化处理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 为了消除不同特征之间量纲的影响，将特征都统一到相同的数值区间中。比如需要评估身高和体重对于健康的影响，如果分别使用米和kg作为单位，那么身高会在1.6到1.8之间，而体重则可能在40到80之间，得到的结果会更偏向于数值差别大的体重。\n",
    "+ 并且进行归一化处理可以在梯度下降过程中更快找到最优的参数，模型在数值区间范围更广的特征上需要更多次的迭代找到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用的归一化方法有哪些？它们的缺点是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用的方法有Min-Max Scaling以及Z-Score。\n",
    "\n",
    "Min-Max Scaling $$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "该方法将数据归一化到$[0,1]$的范围中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Score $$x_{norm} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "该方法将数据归一化到标准正态分布上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max容易受到异常值的影响，使得数据往异常值的方向偏移。假设数据中大部分都是10到20范围之间，但是有少部分数据在0到1之间，那么就会导致归一化后数据整体往0到1的范围偏移。而Z-Score有除以了标准差所以使得数据不会受到异常值的影响。\n",
    "\n",
    "而Z-Score其实是使用样本的均值和方差进行计算，可能存在偏差。其次，Z-Score将数据归一化到了标准正太分布上，因此其实Z-Score更适合于服从正太分布的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 是否所有模型都要求进行特征归一化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不是，通常来说使用梯度下降算法的模型需要归一化。比如，线性回归，逻辑回归，神经网络模型等。但是对于一些模型则不需要，比如决策树模型。这是因为决策树模型在选择最优属性时使用的信息增益（ID3）、信息增益率（C4.5）或者基尼指数都与数据的范围无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别型特征进行编码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 序号编码：通常用于处理类别之间有大小关系的特征。\n",
    "+ onehot编码：适用于处理类别之间没有大小关系的特征。但是当类别取值的个数太多时将带来高维度问题，将需要大量的空间存储数据。此时可以考虑使用稀疏向量存储特征，或者考虑使用二进制编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将两个一维离散特征组合成一个特征，从而可以挖掘多个特征共同对目标的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本特征表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ one-hot encoding: 当单词数量多的时候，特征的维度会特别大。特别的，对于短文本往往还会有稀疏问题。而且onehot只能表达词语出现还是不出现，无法评估不同的词语在一个文章的不同重要程度。\n",
    "+ term-frequency: $\\frac{词语在文章出现的次数}{文章中的总词语数量}$。 TF的一个问题就是对于一些常用的词语，比如“我”，“的”，“是”等在文章中出现的次数比较多，所以TF的值也相对高，但是这些词语并没有很重要的意义，因为它们都是常见的词语无法表达文章的特别性，因此有了TF-IDF。\n",
    "+ TF-IDF: $$tf * idf$$ $$idf(w) = \\frac{总文章数}{包含词语w的文章数+1}$$\n",
    "idf可以表示词语在整个数据集中的重要性，如果包含词语w的文章数多，那么其idf就小。\n",
    "+ 主题模型：使用主题的分布来表示一个文章。缺点是在大数据集上训练耗时。\n",
    "+ 词嵌入：将词语表示成一个n维实数向量，这样以方便可以降低特征维度，另方面可以用两个向量距离的远近则表示它们对应的词语的语义相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习词嵌入的模型有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec, glove, fasttext, ELMo模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何评估词嵌入的好坏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 词语相似度任务：给定两个词语计算他们的相似度。wordsim-297和wordsim-240。\n",
    "+ 词语类比推理：国王-皇后可以类比男人-女人，这样可以在给定国王、男人、女人三个词语之间评估模型推断女人这个单词的准确率。\n",
    "+ 做一些文章分类等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词嵌入训练方法最经典的是word2vec模型。这个模型共有两种方法，一种是CBOW，一种是Skip-gram。前者使用上下文词语预测当前词语，后者则使用当前词语预测上下文词语。\n",
    "\n",
    "模型的学习算法有hierarchical softmax和负采样。前者在输出层是根据词语的频次构造的哈弗曼树，给定了上下文的向量之后，从树的根节点到叶子节点的路径其实都是在做二分类。后者的输出层则是目标词语和负采样得到词语构成的子样本。给定了上下文的向量以及样本集中的一个词语，其实也是在做二分类，目标词语是正类，而负样本则是负类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用负采样方法的好处是：\n",
    "+ 提高训练速度，它不需要构造哈弗曼树，而且哈夫曼树中长度往往大于负采样中负样本的个数。\n",
    "+ 负采样可以通过预先构造映射关系来提高速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据我以前的测试结果来看，word2vec方法中负采样方法得到的结果反而比hs更好，但是word2vec在情感词语上的表现却不太好。比如相同的句子结构，“我喜欢吃西瓜”和“我讨厌吃西瓜”，对于“喜欢”和“讨厌”两个情感极性相反的词语，他们的句子结构和上下文词语一样，因此往往出现情感词性相反的词语它们之间的语义相似度反而高。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$accuracy = \\frac{正确分类的样本}{总样本数}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率计算简单，但是并不适用于类别不平衡的数据。因为，假设模型将所有样本直接预测为类别多的样本，那么准确率也会很高，但是实际上这个模型确是很差的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精确率/召回率/F-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$F1-Score = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n",
    "$$F_{\\beta}-Score = \\frac{(1+\\beta^2) * Precision * Recall}{\\beta^2 * Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 精确率表示模型预测为正类的样本中，有多少是正类的。但是这个指标并不适用于类别不平衡的数据，假设模型直接预测每个样本的类别为数据量多的类别，那么精确度会很高，但是无法反映模型的好坏。\n",
    "+ 召回率表示确实为正类的样本有多少被正确预测。但是召回率高也并不一定是好的，考虑检索场景下，假设返回了尽可能多的结，这样召回率高但是可能返回的结果与搜索关键词的相关度都并不是很高，从而导致精确率很低。所以往往需要综合考虑这两个指标。\n",
    "+ F1-Score是一个等价综合考虑精确率和召回率的指标。$F_\\beta-Score$则可以通过调节$\\beta$的大小来调整召回率相对精确率的重要性，当$\\beta$大于1时则召回率更重要，反之当$\\beta$小于1时则精确率更重要。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC曲线 vs ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 余弦距离及其应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不平衡数据如何选择评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 使用ROC曲线而不是PR曲线作为评估准则，因为ROC曲线不会受到类别不平衡的影响。\n",
    "+ 使用Precison/Recall/F-Score作为综合评估准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数推导： 矩阵直接求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数推导：梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1正则化和l2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从概率的角度解释l1正则化和l2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然和对数损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类和softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续值的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booting集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.619px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
