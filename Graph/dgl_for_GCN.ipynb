{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cora-Dataset\" data-toc-modified-id=\"Cora-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cora Dataset</a></span></li><li><span><a href=\"#使用GraphConv定义GCN\" data-toc-modified-id=\"使用GraphConv定义GCN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>使用GraphConv定义GCN</a></span></li><li><span><a href=\"#使用builtin定义GCN\" data-toc-modified-id=\"使用builtin定义GCN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>使用builtin定义GCN</a></span></li><li><span><a href=\"#模型训练\" data-toc-modified-id=\"模型训练-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>模型训练</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用DGL实现GCN (Graph Convolutional Network)模型，在Cora数据集上完成节点分类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl.data.citation_graph as citegrh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    val_mask = th.BoolTensor(data.val_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    \n",
    "    return g, features, labels, train_mask, val_mask, test_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, features, labels, train_mask, val_mask, test_mask = load_cora_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes, feature_dims = features.shape\n",
    "num_nodes, feature_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GraphConv定义GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在DGL中可以使用多种方法定义GCN，包括预定义的GraphConv类，send/recv接口，builtin接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCNGraphConv, self).__init__()\n",
    "        self._conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self._conv2 = GraphConv(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        h = self._conv1(g, features)\n",
    "        h = F.relu(h)\n",
    "        h = self._conv2(g, h)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "model_graph_conv = GCNGraphConv(feature_dims, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用builtin定义GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dgl整个框架的设计理念其实是将图神经网络看做是一个信息传递(Message Passing)的过程，每一个节点向邻居节点发送信息，同时也接受来自于其他节点的信息，并且将这些信息聚合然后更新自己的状态。每个节点向其他节点发送信息，在dgl内部对应`message_func`，节点汇聚信息则对应`reduce_func`。`builtin`是dgl内部用于加快速度封住的`message_func`函数和`reduce_func`函数。上一节使用dgl提供的`GraphConv`定义了GCN模型，其实`GraphConv`内部的实现也是基于这个思想实现的。\n",
    "\n",
    "在GCN中，每一个节点的状态更新使用如下公式定义：\n",
    "$$h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{(l)}W^{(l)}) = \n",
    "\\sigma(b^{(l)} + W^{(l)}\\frac{1}{\\sqrt{|\\mathcal{N}(i)|}}\\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{\\sqrt{|\\mathcal{N}(j)|}}h_j^{(l)})$$\n",
    "\n",
    "其中，$\\mathcal{N}(i)$表示与节点$i$相连的节点个数，$c_{ij}$则等于$\\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$，起到了归一化的作用。权重$W^{(l)}$使用Glorot均匀分布初始化，偏置$b^{(l)}$初始化为0，$\\sigma$是一个激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvMsgPass(nn.Module):\n",
    "    \"\"\"Define a Graph Convoutional Layer whose weights are initialized \n",
    "    with Glorot uniform distribution, bias are initialized to zero,\n",
    "    and without activation function.\"\"\"\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GraphConvMsgPass, self).__init__()\n",
    "        self._in_feats = in_feats  # size of input features\n",
    "        self._out_feats = out_feats  # sizse of output features\n",
    "        \n",
    "        # initialize weights with Glorot uniform distribution\n",
    "        self._weights = nn.Parameter(th.Tensor(in_feats, out_feats))\n",
    "        init.xavier_uniform_(self._weights)\n",
    "        \n",
    "        # initialize bias to zero\n",
    "        self._bias = nn.Parameter(th.Tensor(out_feats))\n",
    "        init.zeros_(self._bias)\n",
    "    \n",
    "    def forward(self, graph, features):\n",
    "        # calculate out degrees for each node -> N(i)\n",
    "        degs = graph.out_degrees().float().clamp(1)\n",
    "        # 1/sqrt(deg)\n",
    "        norm = th.pow(degs, -0.5)\n",
    "        norm = th.reshape(norm, degs.shape + (1,))\n",
    "        # h_j * (1 / sqrt(N(j)))\n",
    "        features = features * norm\n",
    "        \n",
    "        graph.srcdata['h'] = features\n",
    "        graph.update_all(message_func=fn.copy_src(src='h', out='m'),\n",
    "                         reduce_func=fn.sum(msg='m', out='h'))\n",
    "        rst = graph.dstdata['h']\n",
    "        \n",
    "        # weight\n",
    "        rst = th.matmul(rst, self._weights)\n",
    "        \n",
    "        # normalization -> 1/sqrt(N(i))\n",
    "        rst = rst * norm\n",
    "        \n",
    "        # bias\n",
    "        rst = rst + self._bias\n",
    "        \n",
    "        return rst\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGraphMsgPass(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCNGraphMsgPass, self).__init__()\n",
    "        self._conv1 = GraphConvMsgPass(in_feats, hidden_size)\n",
    "        self._conv2 = GraphConvMsgPass(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        h = self._conv1(g, features)\n",
    "        h = F.relu(h)\n",
    "        h = self._conv2(g, h)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "model_graph_msg = GCNGraphMsgPass(feature_dims, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, g, features, labels, mask):\n",
    "    logits = model(g, features)\n",
    "    logits = F.log_softmax(logits, dim=1)\n",
    "    predict = th.argmax(logits, dim=1)\n",
    "    acc = th.sum(labels[mask] == predict[mask])\n",
    "\n",
    "    return acc.item() / len(labels[mask]) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, max_epochs=50, val_interval=5):\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        logits = model(g, features)\n",
    "        logits = F.log_softmax(logits, dim=1)\n",
    "        loss = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'+++ Epoch = {epoch}, loss = {loss.item():.4f}')\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            acc = validate(model, g, features, labels, val_mask)\n",
    "            print(f'+++ accuracy for validation dataset is {acc:.4f}')\n",
    "            \n",
    "    test_acc = validate(model, g, features, labels, test_mask)\n",
    "    print(f'++++ accuracy for test dataset is {test_acc:.4f}')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Epoch = 0, loss = 0.0099\n",
      "+++ Epoch = 1, loss = 0.0137\n",
      "+++ Epoch = 2, loss = 0.0101\n",
      "+++ Epoch = 3, loss = 0.0110\n",
      "+++ Epoch = 4, loss = 0.0121\n",
      "+++ accuracy for validation dataset is 0.7633\n",
      "+++ Epoch = 5, loss = 0.0111\n",
      "+++ Epoch = 6, loss = 0.0100\n",
      "+++ Epoch = 7, loss = 0.0101\n",
      "+++ Epoch = 8, loss = 0.0108\n",
      "+++ Epoch = 9, loss = 0.0110\n",
      "+++ accuracy for validation dataset is 0.7233\n",
      "+++ Epoch = 10, loss = 0.0106\n",
      "+++ Epoch = 11, loss = 0.0100\n",
      "+++ Epoch = 12, loss = 0.0099\n",
      "+++ Epoch = 13, loss = 0.0102\n",
      "+++ Epoch = 14, loss = 0.0105\n",
      "+++ accuracy for validation dataset is 0.7600\n",
      "+++ Epoch = 15, loss = 0.0104\n",
      "+++ Epoch = 16, loss = 0.0101\n",
      "+++ Epoch = 17, loss = 0.0099\n",
      "+++ Epoch = 18, loss = 0.0100\n",
      "+++ Epoch = 19, loss = 0.0101\n",
      "+++ accuracy for validation dataset is 0.7367\n",
      "+++ Epoch = 20, loss = 0.0102\n",
      "+++ Epoch = 21, loss = 0.0101\n",
      "+++ Epoch = 22, loss = 0.0099\n",
      "+++ Epoch = 23, loss = 0.0099\n",
      "+++ Epoch = 24, loss = 0.0100\n",
      "+++ accuracy for validation dataset is 0.7533\n",
      "+++ Epoch = 25, loss = 0.0101\n",
      "+++ Epoch = 26, loss = 0.0100\n",
      "+++ Epoch = 27, loss = 0.0099\n",
      "+++ Epoch = 28, loss = 0.0099\n",
      "+++ Epoch = 29, loss = 0.0099\n",
      "+++ accuracy for validation dataset is 0.7333\n",
      "+++ Epoch = 30, loss = 0.0100\n",
      "+++ Epoch = 31, loss = 0.0100\n",
      "+++ Epoch = 32, loss = 0.0099\n",
      "+++ Epoch = 33, loss = 0.0099\n",
      "+++ Epoch = 34, loss = 0.0099\n",
      "+++ accuracy for validation dataset is 0.7567\n",
      "+++ Epoch = 35, loss = 0.0100\n",
      "+++ Epoch = 36, loss = 0.0100\n",
      "+++ Epoch = 37, loss = 0.0099\n",
      "+++ Epoch = 38, loss = 0.0099\n",
      "+++ Epoch = 39, loss = 0.0099\n",
      "+++ accuracy for validation dataset is 0.7300\n",
      "+++ Epoch = 40, loss = 0.0099\n",
      "+++ Epoch = 41, loss = 0.0099\n",
      "+++ Epoch = 42, loss = 0.0099\n",
      "+++ Epoch = 43, loss = 0.0099\n",
      "+++ Epoch = 44, loss = 0.0099\n",
      "+++ accuracy for validation dataset is 0.7500\n",
      "+++ Epoch = 45, loss = 0.0099\n",
      "+++ Epoch = 46, loss = 0.0099\n",
      "+++ Epoch = 47, loss = 0.0099\n",
      "+++ Epoch = 48, loss = 0.0099\n",
      "+++ Epoch = 49, loss = 0.0099\n",
      "+++ accuracy for validation dataset is 0.7333\n",
      "++++ accuracy for test dataset is 0.7780\n"
     ]
    }
   ],
   "source": [
    "train_model(model_graph_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Epoch = 0, loss = 1.9455\n",
      "+++ Epoch = 1, loss = 1.8626\n",
      "+++ Epoch = 2, loss = 1.7700\n",
      "+++ Epoch = 3, loss = 1.6800\n",
      "+++ Epoch = 4, loss = 1.6054\n",
      "+++ accuracy for validation dataset is 0.4667\n",
      "+++ Epoch = 5, loss = 1.5224\n",
      "+++ Epoch = 6, loss = 1.4174\n",
      "+++ Epoch = 7, loss = 1.3029\n",
      "+++ Epoch = 8, loss = 1.1911\n",
      "+++ Epoch = 9, loss = 1.0832\n",
      "+++ accuracy for validation dataset is 0.7167\n",
      "+++ Epoch = 10, loss = 0.9752\n",
      "+++ Epoch = 11, loss = 0.8670\n",
      "+++ Epoch = 12, loss = 0.7619\n",
      "+++ Epoch = 13, loss = 0.6632\n",
      "+++ Epoch = 14, loss = 0.5725\n",
      "+++ accuracy for validation dataset is 0.8033\n",
      "+++ Epoch = 15, loss = 0.4900\n",
      "+++ Epoch = 16, loss = 0.4162\n",
      "+++ Epoch = 17, loss = 0.3517\n",
      "+++ Epoch = 18, loss = 0.2968\n",
      "+++ Epoch = 19, loss = 0.2509\n",
      "+++ accuracy for validation dataset is 0.8133\n",
      "+++ Epoch = 20, loss = 0.2131\n",
      "+++ Epoch = 21, loss = 0.1824\n",
      "+++ Epoch = 22, loss = 0.1573\n",
      "+++ Epoch = 23, loss = 0.1368\n",
      "+++ Epoch = 24, loss = 0.1199\n",
      "+++ accuracy for validation dataset is 0.8267\n",
      "+++ Epoch = 25, loss = 0.1060\n",
      "+++ Epoch = 26, loss = 0.0944\n",
      "+++ Epoch = 27, loss = 0.0848\n",
      "+++ Epoch = 28, loss = 0.0768\n",
      "+++ Epoch = 29, loss = 0.0701\n",
      "+++ accuracy for validation dataset is 0.8133\n",
      "+++ Epoch = 30, loss = 0.0644\n",
      "+++ Epoch = 31, loss = 0.0596\n",
      "+++ Epoch = 32, loss = 0.0555\n",
      "+++ Epoch = 33, loss = 0.0520\n",
      "+++ Epoch = 34, loss = 0.0490\n",
      "+++ accuracy for validation dataset is 0.8033\n",
      "+++ Epoch = 35, loss = 0.0463\n",
      "+++ Epoch = 36, loss = 0.0438\n",
      "+++ Epoch = 37, loss = 0.0416\n",
      "+++ Epoch = 38, loss = 0.0396\n",
      "+++ Epoch = 39, loss = 0.0378\n",
      "+++ accuracy for validation dataset is 0.8067\n",
      "+++ Epoch = 40, loss = 0.0362\n",
      "+++ Epoch = 41, loss = 0.0347\n",
      "+++ Epoch = 42, loss = 0.0334\n",
      "+++ Epoch = 43, loss = 0.0323\n",
      "+++ Epoch = 44, loss = 0.0312\n",
      "+++ accuracy for validation dataset is 0.8133\n",
      "+++ Epoch = 45, loss = 0.0302\n",
      "+++ Epoch = 46, loss = 0.0292\n",
      "+++ Epoch = 47, loss = 0.0283\n",
      "+++ Epoch = 48, loss = 0.0275\n",
      "+++ Epoch = 49, loss = 0.0267\n",
      "+++ accuracy for validation dataset is 0.8200\n",
      "++++ accuracy for test dataset is 0.8050\n"
     ]
    }
   ],
   "source": [
    "train_model(model_graph_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
