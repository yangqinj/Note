{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本预处理（English Text Preprocessing）\n",
    "英文文本的数据预处理主要分为三大部分：数据收集和整理，分词和归一化。数据收集和整理通常是使用数据库，文本文件或者从网站上抓取的原始文本(raw data)，这里我们直接从sklearn中获取文本数据[20_news](http://qwone.com/~jason/20Newsgroups/)。分词指将文本划分为一个个的单词。英文的单词划分较为简单，通常通过检测句末标点完成。归一化的内容多而杂，且没有固定的先后顺序标准，以及处理方法标准，归一化的流程要根据具体的任务场景来分析，这部分往往是很花费时间的。归一化通常包括：去除噪声，大小写转换，去除停用词，词形还原，词干还原，拼写检查和词性标注等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据收集和整理（Data Collection and Assembly）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " rec.autos\n",
      "Lines:  22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='train')\n",
    "text, categories = news.data, news.target\n",
    "single_text = text[0]\n",
    "category = categories[0]\n",
    "print(single_text, news.target_names[category])\n",
    "print('Lines: ', len(single_text.split('\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is  all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail.  Thanks, - IL    ---- brought to you by your neighborhood Lerxst ----     \n"
     ]
    }
   ],
   "source": [
    "lines = single_text.split('\\n')\n",
    "num_lines = int(lines[4].split(': ')[-1])\n",
    "# print(num_lines)\n",
    "cont = ' '.join(lines[-num_lines-1:])\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "+ **Sentence Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for original text:\n",
      " [\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\", 'Nntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day.', 'It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s.', 'It was called a Bricklin.', 'The doors were really small.', 'In addition,\\nthe front bumper was separate from the rest of the body.', 'This is \\nall I know.', 'If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.', 'Thanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      "Sentences for only content:\n",
      " [' I was wondering if anyone out there could enlighten me on this car I saw the other day.', 'It was a 2-door sports car, looked to be from the late 60s/ early 70s.', 'It was called a Bricklin.', 'The doors were really small.', 'In addition, the front bumper was separate from the rest of the body.', 'This is  all I know.', 'If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail.', 'Thanks, - IL    ---- brought to you by your neighborhood Lerxst ----     ']\n",
      "\n",
      "\n",
      "Sentences for sample\n",
      " [' I was wondering if anyone out there could enlighten me on this car I saw the other day.', 'It was a 2-door sports car, looked to be from the late 60s/ early 70s.', 'It was called a Bricklin.', 'The doors were really small.', 'In addition, the front bumper was separate from the rest of the body.', 'This is  all I know.', 'If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail.', 'Thanks, - IL    ---- brought to you by your neighborhood Lerxst ----     ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# split sentences with original single_text\n",
    "sents = sent_tokenize(single_text)\n",
    "print('Sentences for original text:\\n', sents)\n",
    "# split sentences with extracted and format content\n",
    "sents = sent_tokenize(cont)\n",
    "print('\\n\\nSentences for only content:\\n', sents)\n",
    "\n",
    "sample = 'This is Mr.Li.'\n",
    "sample_sents = sent_tokenize(sample)\n",
    "print('\\n\\nSentences for sample\\n', sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------\n",
    "观察结果可以发现，句子分割的结果根据文本中是否检测到句末标点决定。可以检测到几种特殊的场景，比如邮件地址（lerxst@wam.umd.edu），连续的句末标点（WHAT car is this!?），以及一些缩写（This is Mr.Li.）\n",
    "\n",
    "+ **Word Toenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for whole content:\n",
      " ['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n",
      "\n",
      "\n",
      "Words for each sentences:\n",
      "\n",
      "['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.']\n",
      "['It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.']\n",
      "['It', 'was', 'called', 'a', 'Bricklin', '.']\n",
      "['The', 'doors', 'were', 'really', 'small', '.']\n",
      "['In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.']\n",
      "['This', 'is', 'all', 'I', 'know', '.']\n",
      "['If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n",
      "['Thanks', ',', '-', 'IL', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n",
      "\n",
      "\n",
      "\n",
      "['They', \"'ll\", 'can', 'not', 'could', \"n't\", 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(cont)\n",
    "print('Words for whole content:\\n', words)\n",
    "\n",
    "print('\\n\\nWords for each sentences:\\n')\n",
    "for sent in sents:\n",
    "    words = word_tokenize(sent)\n",
    "    print(words)\n",
    "\n",
    "sample = \"They'll cannot couldn't New York\"\n",
    "print('\\n\\n')\n",
    "print(word_tokenize(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--------\n",
    "从结果中可以观察到：\n",
    "+ 单条句子的分词和整个文本的分词结果是一样的。如果只是进行篇章级别的分类等任务，其实没有必要单独对句子进行处理。\n",
    "+ 对于某些缩写，分词只是简单的将单词分开而已，并不会恢复单词原型。其实这样做是合理的，因为这些可以被缩写的词通常并没有实际语义，比如will。\n",
    "+ 虽然not会影响情感的正负极，但是可以通过额外的处理将其恢复，比如将couldn't 处理为could_not等。\n",
    "+ 标点符号被识别为单个单词。\n",
    "+ 专有名词比如New York被拆分成两个单词，因此分词之前可以先做命名实体识别，对这些单词做额外处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "+ Noise Removal\n",
    "移除掉一些噪声文本，比如html的标签，非英文字符等。\n",
    "+ Spell Check\n",
    "英文拼写检查常用的pyhton库是[pyenchant](https://pypi.org/project/pyenchant/)\n",
    "参考资料:[https://blog.csdn.net/hpulfc/article/details/80997252](https://blog.csdn.net/hpulfc/article/details/80997252)  [https://pythonhosted.org/pyenchant/tutorial.html](https://pythonhosted.org/pyenchant/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error word: Bricklin\n",
      "Error word: tellme\n",
      "Error word: Lerxst\n"
     ]
    }
   ],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"en_US\") # 使用美式英语\n",
    "chkr.set_text(cont)\n",
    "for err in chkr:\n",
    "    print(\"Error word: %s\" % (err.word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果中可以观察到：可能将人名识别为错误单词。可以考虑增加一个过滤单词列表，检查完成后筛选词语。\n",
    "+ **POS（词性标注）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('wondering', 'VBG'), ('if', 'IN'), ('anyone', 'NN'), ('out', 'IN'), ('there', 'RB'), ('could', 'MD'), ('enlighten', 'VB'), ('me', 'PRP'), ('on', 'IN'), ('this', 'DT'), ('car', 'NN'), ('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('other', 'JJ'), ('day', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "sent = sents[0]\n",
    "words = word_tokenize(sent)\n",
    "words_tags = pos_tag(words)\n",
    "print(words_tags)\n",
    "# tags对应的词性可以查看http://www.nltk.org/book/ch05.html#tab-universal-tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "+ **Stemming(词干提取) / Lemmatization（词形还原）**\n",
    "\n",
    "Stemming(词干提取)用于提取一个词的词干，可能得到的%debug果其实并不是一个词。Lemmatization（词形还原）用于将一个词从正在进行时，过去式等还原。通常采用先词形还原后词干提取，归一化不同词性的单词。\n",
    "\n",
    "WordNet词形还原方法是常用的算法，如果一个词的后缀移除后所的单词在它的词库里，就移除这个词的后缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of simple use: \n",
      " ['I', 'wa', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.']\n",
      "\n",
      "\n",
      "result of with pos info:\n",
      " ['I', 'be', 'wonder', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.']\n",
      "\n",
      "\n",
      "result of sample: \n",
      " ['It', 'take', 'me', 'several', 'minute', 'in', 'reading', 'the', 'news', 'about', 'mathematics']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 1. 最简单的使用方法\n",
    "print ('result of simple use: \\n', [lemmatizer.lemmatize(w) for w in words])\n",
    "\n",
    "# 2. 加上了POS信息\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # default is NOUN\n",
    "\n",
    "res = []\n",
    "for word, pos in words_tags:\n",
    "    wordnet_pos = get_wordnet_pos(pos)\n",
    "    res.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
    "print('\\n\\nresult of with pos info:\\n', res)\n",
    "\n",
    "sample_text = 'It takes me several minutes in reading the news about mathematics'\n",
    "print ('\\n\\nresult of sample: \\n', [lemmatizer.lemmatize(w) for w in word_tokenize(sample_text)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "从上述结果中，我们可以观察到：\n",
    "+ 没加pos信息之前，wordnet无法还原单词变化较大的过去式（was->wa, saw->saw），并且从侧面验证了wordnet只是比较词语去掉后缀后是否可以构成新单词\n",
    "+ 没加pos信息之前，wordnet还无法还原动词正在进行时（wondering -> wondering，wondering的词性是动词，wordnet知道这个信息就尝试还原词行)。\n",
    "+ wordnet可以还原单词复数(minutes -> minute)\n",
    "+ 如果单词的复数已经存在于词库中，就不会还原（news -> new），无法还原正在进行时应该也是这个原因\n",
    "\n",
    "-----\n",
    "最常用的词干化工具是porter stemmer，但是网上有人推荐使用snowball stemmer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      " ['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n",
      "\n",
      "result for use PorterStemmer:\n",
      " ['I', 'wa', 'wonder', 'if', 'anyon', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'thi', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'wa', 'a', '2-door', 'sport', 'car', ',', 'look', 'to', 'be', 'from', 'the', 'late', '60s/', 'earli', '70', '.', 'It', 'wa', 'call', 'a', 'Bricklin', '.', 'The', 'door', 'were', 'realli', 'small', '.', 'In', 'addit', ',', 'the', 'front', 'bumper', 'wa', 'separ', 'from', 'the', 'rest', 'of', 'the', 'bodi', '.', 'Thi', 'is', 'all', 'I', 'know', '.', 'If', 'anyon', 'can', 'tellm', 'a', 'model', 'name', ',', 'engin', 'spec', ',', 'year', 'of', 'product', ',', 'where', 'thi', 'car', 'is', 'made', ',', 'histori', ',', 'or', 'whatev', 'info', 'you', 'have', 'on', 'thi', 'funki', 'look', 'car', ',', 'pleas', 'e-mail', '.', 'Thank', ',', '-', 'IL', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n",
      "\n",
      "result for use SnowballStemmer:\n",
      " ['i', 'was', 'wonder', 'if', 'anyon', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sport', 'car', ',', 'look', 'to', 'be', 'from', 'the', 'late', '60s/', 'earli', '70s', '.', 'it', 'was', 'call', 'a', 'bricklin', '.', 'the', 'door', 'were', 'realli', 'small', '.', 'in', 'addit', ',', 'the', 'front', 'bumper', 'was', 'separ', 'from', 'the', 'rest', 'of', 'the', 'bodi', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyon', 'can', 'tellm', 'a', 'model', 'name', ',', 'engin', 'spec', ',', 'year', 'of', 'product', ',', 'where', 'this', 'car', 'is', 'made', ',', 'histori', ',', 'or', 'whatev', 'info', 'you', 'have', 'on', 'this', 'funki', 'look', 'car', ',', 'pleas', 'e-mail', '.', 'thank', ',', '-', 'il', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst', '--', '--']\n",
      "\n",
      "result for use LancasterStemmer:\n",
      " ['i', 'was', 'wond', 'if', 'anyon', 'out', 'ther', 'could', 'enlight', 'me', 'on', 'thi', 'car', 'i', 'saw', 'the', 'oth', 'day', '.', 'it', 'was', 'a', '2-door', 'sport', 'car', ',', 'look', 'to', 'be', 'from', 'the', 'lat', '60s/', 'ear', '70s', '.', 'it', 'was', 'cal', 'a', 'bricklin', '.', 'the', 'door', 'wer', 'real', 'smal', '.', 'in', 'addit', ',', 'the', 'front', 'bump', 'was', 'sep', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'thi', 'is', 'al', 'i', 'know', '.', 'if', 'anyon', 'can', 'tellm', 'a', 'model', 'nam', ',', 'engin', 'spec', ',', 'year', 'of', 'produc', ',', 'wher', 'thi', 'car', 'is', 'mad', ',', 'hist', ',', 'or', 'whatev', 'info', 'you', 'hav', 'on', 'thi', 'funky', 'look', 'car', ',', 'pleas', 'e-mail', '.', 'thank', ',', '-', 'il', '--', '--', 'brought', 'to', 'you', 'by', 'yo', 'neighb', 'lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "cont_words = word_tokenize(cont)\n",
    "print(\"Words:\\n\", cont_words)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "res = [stemmer.stem(w) for w in cont_words]\n",
    "print(\"\\nresult for use PorterStemmer:\\n\", res)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "res = [stemmer.stem(w) for w in cont_words]\n",
    "print(\"\\nresult for use SnowballStemmer:\\n\", res)\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "res = [stemmer.stem(w) for w in cont_words]\n",
    "print(\"\\nresult for use LancasterStemmer:\\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 总体来说，激进程度`LancasterStemmer > PorterStemmer > SnowballStemmer`，比如`wondering -> wonder/wonder/wond, late -> late/late/lat, bumper-> bumper/bumper/bump， neighborhood -> neighborhood/neighborhood/neighb`\n",
    "+ `LancasterStemmer`太激进，很多正常的单词被还原太多，下面不做分析。\n",
    "+ `SnowballStemmer`相比`PorterStemmer`较为温和，一些常用的词语不会还原，比如`this, there, was`等\n",
    "+ `SnowballStemmer`可以适用于多种语言，并且可以指定是否使用过滤词表\n",
    "+ `SnowballStemmer`还会将词语转化为小写。\n",
    "\n",
    "----\n",
    "+ ** 去除停用词**\n",
    "\n",
    "停用词指对于当前任务场景一些无意义的词语，比如对于大多数场景而言was, the, that等都是无意义词语。但是对于情感分析而言，that却可以指代情感的对象，比如一部手机，一个杯子等。因此没有所谓通用的停用词，停用词列表需要针对特定场景使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words:\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
      "\n",
      "\n",
      "number of words before filtering is 121\n",
      "\n",
      "Filter words:\n",
      " ['I', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'I', 'saw', 'day', '.', 'It', '2-door', 'sports', 'car', ',', 'looked', 'late', '60s/', 'early', '70s', '.', 'It', 'called', 'Bricklin', '.', 'The', 'doors', 'really', 'small', '.', 'In', 'addition', ',', 'front', 'bumper', 'separate', 'rest', 'body', '.', 'This', 'I', 'know', '.', 'If', 'anyone', 'tellme', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'production', ',', 'car', 'made', ',', 'history', ',', 'whatever', 'info', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'brought', 'neighborhood', 'Lerxst', '--', '--']\n",
      "number of words after filtering is 80\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(\"stop_words:\\n\", stop_words)\n",
    "\n",
    "print(\"\\n\\nnumber of words before filtering is %d\" % len(cont_words))\n",
    "filter_words = [w for w in cont_words if w not in stop_words]\n",
    "print(\"\\nFilter words:\\n\", filter_words)\n",
    "print(\"number of words after filtering is %d\" % len(filter_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
