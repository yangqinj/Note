{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192 ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n",
      "57340 [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'], ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.'], ['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.'], ['It', 'recommended', 'that', 'Fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.'], ['The', 'grand', 'jury', 'commented', 'on', 'a', 'number', 'of', 'other', 'topics', ',', 'among', 'them', 'the', 'Atlanta', 'and', 'Fulton', 'County', 'purchasing', 'departments', 'which', 'it', 'said', '``', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'to', 'the', 'best', 'interest', 'of', 'both', 'governments', \"''\", '.'], ['Merger', 'proposed'], ['However', ',', 'the', 'jury', 'said', 'it', 'believes', '``', 'these', 'two', 'offices', 'should', 'be', 'combined', 'to', 'achieve', 'greater', 'efficiency', 'and', 'reduce', 'the', 'cost', 'of', 'administration', \"''\", '.'], ['The', 'City', 'Purchasing', 'Department', ',', 'the', 'jury', 'said', ',', '``', 'is', 'lacking', 'in', 'experienced', 'clerical', 'personnel', 'as', 'a', 'result', 'of', 'city', 'personnel', 'policies', \"''\", '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "corpus_words = [w.lower() for w in brown.words()]\n",
    "print(len(corpus_words), corpus_words[:10])\n",
    "\n",
    "corpus_sents = brown.sents()\n",
    "print(len(corpus_sents), corpus_sents[:10])\n",
    "\n",
    "words = corpus_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram and data smooth algorithm\n",
    "\n",
    "Practice ngram models in nltk and several smooth algorithms, including the simplest additive smooth, Knerser Ney algorithm.\n",
    "\n",
    "## N-gram \n",
    "+ Using **_ngrams function_** in nltk to extract n-grams tuple from given word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ngrams function\n",
      "n=1:  [('the',), ('fulton',), ('county',), ('grand',), ('jury',), ('said',), ('friday',), ('an',), ('investigation',), ('of',)]\n",
      "\n",
      "n=2:  [('the', 'fulton'), ('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'an'), ('an', 'investigation'), ('investigation', 'of')]\n",
      "\n",
      "n=3:  [('the', 'fulton', 'county'), ('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'an'), ('friday', 'an', 'investigation'), ('an', 'investigation', 'of')]\n",
      "\n",
      "n=2 with left and right padding:\n",
      "[('<s>', 'the'), ('the', 'fulton'), ('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', '</s>')]\n",
      "\n",
      "Results for bigrams function\n",
      "[('the', 'fulton'), ('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'an'), ('an', 'investigation'), ('investigation', 'of')]\n",
      "\n",
      "Results for trigrams function\n",
      "[('the', 'fulton', 'county'), ('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'an'), ('friday', 'an', 'investigation'), ('an', 'investigation', 'of')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams,bigrams, trigrams\n",
    "print(\"Results for ngrams function\")\n",
    "print(\"n=1: \", list(ngrams(words, n=1)))\n",
    "print(\"\\nn=2: \", list(ngrams(words, n=2)))\n",
    "print(\"\\nn=3: \", list(ngrams(words, n=3)))\n",
    "\n",
    "print(\"\\nn=2 with left and right padding:\")\n",
    "print(list(ngrams(words, n=2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')))\n",
    "\n",
    "print(\"\\nResults for bigrams function\")\n",
    "print(list(bigrams(words)))\n",
    "\n",
    "print(\"\\nResults for trigrams function\")\n",
    "print(list(trigrams(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Smooth in NLTK\n",
    "Using **_N-gram Module_** and related modules in NLTK to build language model with different data smooth algorithms. Because Knerser-Ney module in NLTK can only be used for trigrams, we extract the trigrams here.\n",
    "\n",
    "Other open source language model tools for Knerser-Ney algorithm can be referenced [http://smithamilli.com/blog/kneser-ney/](http://smithamilli.com/blog/kneser-ney/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Fulton', 'County'), ('Fulton', 'County', 'Grand'), ('County', 'Grand', 'Jury'), ('Grand', 'Jury', 'said'), ('Jury', 'said', 'Friday'), ('said', 'Friday', 'an'), ('Friday', 'an', 'investigation'), ('an', 'investigation', 'of'), ('investigation', 'of', \"Atlanta's\"), ('of', \"Atlanta's\", 'recent')]\n"
     ]
    }
   ],
   "source": [
    "# brown_grams = [gram for sent in corpus_sents[:1000] for gram in ngrams(sent[:-2], 3, pad_left=True, left_pad_symbol='BOS', pad_right=True, right_pad_symbol='EOS')]\n",
    "brown_grams = [gram for sent in corpus_sents[:1000] for gram in ngrams(sent[:-2], 3)]\n",
    "print(brown_grams[:10])\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(brown_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_samples_by_prob(probdist):\n",
    "    sample_prob = dict((s, probdist.prob(s)) for s in probdist.samples())\n",
    "    return sorted(sample_prob.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 highest probability:\n",
      "((',', 'he', 'said'), 0.0008783004885546468)\n",
      "(('the', 'United', 'States'), 0.0008234067080199813)\n",
      "((\"''\", ',', 'he'), 0.0007136191469506505)\n",
      "(('said', ',', '``'), 0.0007136191469506505)\n",
      "(('he', 'said', ','), 0.0007136191469506505)\n",
      "((\"''\", ',', 'the'), 0.0006587253664159851)\n",
      "(('is', 'expected', 'to'), 0.0004940440248119889)\n",
      "(('chairman', 'of', 'the'), 0.00038425646374265796)\n",
      "(('one', 'of', 'the'), 0.00038425646374265796)\n",
      "(('session', 'of', 'the'), 0.00038425646374265796)\n",
      "((',', 'the', 'jury'), 0.00038425646374265796)\n",
      "(('in', 'the', 'state'), 0.00038425646374265796)\n",
      "(('some', 'of', 'the'), 0.00038425646374265796)\n",
      "(('of', 'the', 'Republican'), 0.00038425646374265796)\n",
      "(('that', 'the', 'United'), 0.00032936268320799255)\n",
      "(('Mr.', 'Hawksley', 'said'), 0.00032936268320799255)\n",
      "(('president', 'of', 'the'), 0.00032936268320799255)\n",
      "(('the', 'sales', 'tax'), 0.00032936268320799255)\n",
      "((',', 'it', 'was'), 0.00032936268320799255)\n",
      "(('``', 'This', 'is'), 0.00027446890267332713)\n",
      "top 10 lowest probability:\n",
      "(('car', 'flying', 'over'), 5.489378053466542e-05)\n",
      "(('been', 'fired', 'on'), 5.489378053466542e-05)\n",
      "(('sue', 'for', 'recovery'), 5.489378053466542e-05)\n",
      "(('was', 'a', 'Socialist'), 5.489378053466542e-05)\n",
      "(('City', 'Council', 'and'), 5.489378053466542e-05)\n",
      "(('Barnet', 'Lieberman', 'and'), 5.489378053466542e-05)\n",
      "(('Goldberg', ',', 'attorney'), 5.489378053466542e-05)\n",
      "(('The', 'new', 'fee'), 5.489378053466542e-05)\n",
      "(('and', 'juvenile', 'arrests'), 5.489378053466542e-05)\n",
      "(('resent', 'deeply', 'the'), 5.489378053466542e-05)\n",
      "(('have', 'made', '``'), 5.489378053466542e-05)\n"
     ]
    }
   ],
   "source": [
    "# maximum likelihood\n",
    "from nltk.probability import MLEProbDist\n",
    "mle = MLEProbDist(fdist)\n",
    "sample_prob = sort_samples_by_prob(mle)\n",
    "print(\"top 20 highest probability:\")\n",
    "for t in sample_prob[:20]: print(t)\n",
    "print(\"top 10 lowest probability:\")\n",
    "for t in sample_prob[-11:]: print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n",
      "top 20 highest probability:\n",
      "((',', 'he', 'said'), 0.0004801310475329737)\n",
      "(('the', 'United', 'States'), 0.00045188804473691644)\n",
      "((\"''\", ',', 'he'), 0.00039540203914480187)\n",
      "(('he', 'said', ','), 0.00039540203914480187)\n",
      "(('said', ',', '``'), 0.00039540203914480187)\n",
      "((\"''\", ',', 'the'), 0.0003671590363487446)\n",
      "(('is', 'expected', 'to'), 0.0002824300279605728)\n",
      "(('one', 'of', 'the'), 0.00022594402236845822)\n",
      "(('in', 'the', 'state'), 0.00022594402236845822)\n",
      "(('session', 'of', 'the'), 0.00022594402236845822)\n",
      "(('chairman', 'of', 'the'), 0.00022594402236845822)\n",
      "((',', 'the', 'jury'), 0.00022594402236845822)\n",
      "(('some', 'of', 'the'), 0.00022594402236845822)\n",
      "(('of', 'the', 'Republican'), 0.00022594402236845822)\n",
      "(('that', 'the', 'United'), 0.00019770101957240093)\n",
      "(('Mr.', 'Hawksley', 'said'), 0.00019770101957240093)\n",
      "(('the', 'sales', 'tax'), 0.00019770101957240093)\n",
      "(('president', 'of', 'the'), 0.00019770101957240093)\n",
      "((',', 'it', 'was'), 0.00019770101957240093)\n",
      "(('``', 'This', 'is'), 0.00016945801677634365)\n",
      "top 10 lowest probability:\n",
      "(('If', 'the', 'city'), 5.6486005592114554e-05)\n",
      "(('been', 'fired', 'on'), 5.6486005592114554e-05)\n",
      "(('sue', 'for', 'recovery'), 5.6486005592114554e-05)\n",
      "(('council', 'voted', 'to'), 5.6486005592114554e-05)\n",
      "(('and', 'making', 'New'), 5.6486005592114554e-05)\n",
      "(('care', 'for', 'those'), 5.6486005592114554e-05)\n",
      "(('Goldberg', ',', 'attorney'), 5.6486005592114554e-05)\n",
      "(('the', 'shouting', 'ended'), 5.6486005592114554e-05)\n",
      "(('and', 'outside', 'of'), 5.6486005592114554e-05)\n",
      "(('proposals', 'for', 'new'), 5.6486005592114554e-05)\n",
      "(('here', 'offered', 'a'), 5.6486005592114554e-05)\n"
     ]
    }
   ],
   "source": [
    "# addictive smooth\n",
    "add_fdist = dict(fdist)\n",
    "print(fdist[('the', 'United', 'States')])\n",
    "add_fdist = FreqDist({key: value+1 for key,value in add_fdist.items()})\n",
    "print(add_fdist[('the', 'United', 'States')])\n",
    "\n",
    "mle = MLEProbDist(add_fdist)\n",
    "sample_prob = sort_samples_by_prob(mle)\n",
    "print(\"top 20 highest probability:\")\n",
    "for t in sample_prob[:20]: print(t)\n",
    "print(\"top 10 lowest probability:\")\n",
    "for t in sample_prob[-11:]: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到，进行了加性平滑后高概率的前20个元组的概率都下降了将近一半。说明训练集中词汇表的词汇量较多，从而导致加上的1被平均了很多。而出现概率小的那些词组的概率则上升了一些。并且，这些说明了加法平滑方法的”劫“的程度受到词汇量影响很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount =  0.32453303447839277\n",
      "top 20 highest probability:\n",
      "((',', 'he', 'said'), 0.000709406063961109) 16 1 0\n",
      "(('the', 'United', 'States'), 0.0006583934318078581) 15 1 1\n",
      "((\"''\", ',', 'he'), 0.0005567072141643199) 13 3 0\n",
      "(('he', 'said', ','), 0.0005567072141643199) 13 3 0\n",
      "(('said', ',', '``'), 0.0005567072141643199) 13 3 0\n",
      "((\"''\", ',', 'the'), 0.0005060835398362218) 12 1 3\n",
      "(('is', 'expected', 'to'), 0.00035558725507260815) 9 1 0\n",
      "(('one', 'of', 'the'), 0.00025720007638937444) 7 7 0\n",
      "(('in', 'the', 'state'), 0.00025720007638937444) 7 7 0\n",
      "(('session', 'of', 'the'), 0.00025720007638937444) 7 7 0\n",
      "(('chairman', 'of', 'the'), 0.00025720007638937444) 7 7 0\n",
      "((',', 'the', 'jury'), 0.00025720007638937444) 7 7 0\n",
      "(('some', 'of', 'the'), 0.00025720007638937444) 7 7 0\n",
      "(('of', 'the', 'Republican'), 0.00025720007638937444) 7 7 0\n",
      "(('that', 'the', 'United'), 0.00020904794161914772) 6 5 7\n",
      "(('Mr.', 'Hawksley', 'said'), 0.00020904794161914772) 6 5 7\n",
      "(('the', 'sales', 'tax'), 0.00020904794161914772) 6 5 7\n",
      "(('president', 'of', 'the'), 0.00020904794161914772) 6 5 7\n",
      "((',', 'it', 'was'), 0.00020904794161914772) 6 5 7\n",
      "(('``', 'This', 'is'), 0.00016200709164851656) 5 11 5\n",
      "top 10 lowest probability:\n",
      "(('If', 'the', 'city'), 3.376290301460123e-06) 1 16489 537\n",
      "(('been', 'fired', 'on'), 3.376290301460123e-06) 1 16489 537\n",
      "(('sue', 'for', 'recovery'), 3.376290301460123e-06) 1 16489 537\n",
      "(('was', 'a', 'Socialist'), 3.376290301460123e-06) 1 16489 537\n",
      "(('and', 'making', 'New'), 3.376290301460123e-06) 1 16489 537\n",
      "(('care', 'for', 'those'), 3.376290301460123e-06) 1 16489 537\n",
      "(('Goldberg', ',', 'attorney'), 3.376290301460123e-06) 1 16489 537\n",
      "(('The', 'new', 'fee'), 3.376290301460123e-06) 1 16489 537\n",
      "(('and', 'outside', 'of'), 3.376290301460123e-06) 1 16489 537\n",
      "(('resent', 'deeply', 'the'), 3.376290301460123e-06) 1 16489 537\n",
      "(('here', 'offered', 'a'), 3.376290301460123e-06) 1 16489 537\n"
     ]
    }
   ],
   "source": [
    "# Good Turing Smooth\n",
    "from nltk.probability import SimpleGoodTuringProbDist\n",
    "fdist = FreqDist(fdist)\n",
    "gtp = SimpleGoodTuringProbDist(fdist)\n",
    "print(\"discount = \", gtp.discount())\n",
    "sample_prob = sort_samples_by_prob(gtp)\n",
    "print(\"top 20 highest probability:\")\n",
    "for t in sample_prob[:20]: print(t, fdist[t[0]], fdist.Nr(fdist[t[0]]), fdist.Nr(fdist[t[0]]+1))\n",
    "print(\"top 10 lowest probability:\")\n",
    "for t in sample_prob[-11:]: print(t, fdist[t[0]], fdist.Nr(fdist[t[0]]), fdist.Nr(fdist[t[0]]+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果中可以观察到，对于出现频次高的词组，其概率相比最大似然有所下降，但是下降幅度远小于加法平滑方法。对于出现频次低的词组，其概率相比最大似然也下降了，并且下降的幅度非常大。这是因为出现次数为2的词组的次数是出现次数为1的词组的次数的百分之一（537/16489)。说明：古德图灵平滑方法中词的概率受到$N_{r+1}$的影响很大。但是我认为古德图灵方法只是利用了齐夫定律缩小词的个数，但是$N_r$和$N_{r+1}$之间没有可解释关系，即词出现的次数为什么要受到出现次数比其多1的词的个数影响？这个问题没有解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b6be602d27ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Witten Bell smooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWittenBellProbDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWittenBellProbDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_samples_by_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top 20 highest probability:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, freqdist, bins)\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_P0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_P0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_T\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Z\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Witten Bell smooth\n",
    "from nltk.probability import WittenBellProbDist\n",
    "wb = WittenBellProbDist(fdist)\n",
    "sample_prob = sort_samples_by_prob(wb)\n",
    "print(\"top 20 highest probability:\")\n",
    "for t in sample_prob[:20]: print(t)\n",
    "print(\"top 10 lowest probability:\")\n",
    "for t in sample_prob[-11:]: print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 highest probability:\n",
      "(('chairman', 'of', 'the'), 0.8928571428571429)\n",
      "(('the', 'need', 'for'), 0.85)\n",
      "(('precinct', 'of', 'the'), 0.8125)\n",
      "(('the', 'Citizens', 'Group'), 0.8125)\n",
      "(('a', 'number', 'of'), 0.8125)\n",
      "(('the', 'full', 'amount'), 0.8125)\n",
      "(('aged', 'care', 'plan'), 0.8125)\n",
      "(('be', 'able', 'to'), 0.8125)\n",
      "((',', 'according', 'to'), 0.8125)\n",
      "(('the', 'United', 'States'), 0.7916666666666666)\n",
      "(('session', 'of', 'the'), 0.78125)\n",
      "(('to', 'attend', 'the'), 0.75)\n",
      "(('the', 'views', 'of'), 0.75)\n",
      "(('study', 'of', 'the'), 0.75)\n",
      "(('there', 'would', 'be'), 0.75)\n",
      "(('a', 'series', 'of'), 0.75)\n",
      "((',', 'chairman', 'of'), 0.75)\n",
      "(('the', 'alliance', ','), 0.75)\n",
      "(('headed', 'by', 'the'), 0.75)\n",
      "(('the', 'sales', 'tax'), 0.75)\n",
      "top 10 lowest probability:\n",
      "(('of', 'the', 'Kennedy'), 0.0012195121951219512)\n",
      "(('of', 'the', 'concessionaires'), 0.0012195121951219512)\n",
      "(('of', 'the', 'twenty'), 0.0012195121951219512)\n",
      "(('of', 'the', 'avenue'), 0.0012195121951219512)\n",
      "(('of', 'the', 'board'), 0.0012195121951219512)\n",
      "(('of', 'the', 'hospital-care'), 0.0012195121951219512)\n",
      "(('of', 'the', 'Forest'), 0.0012195121951219512)\n",
      "(('of', 'the', '4-year'), 0.0012195121951219512)\n",
      "(('of', 'the', 'religious'), 0.0012195121951219512)\n",
      "(('of', 'the', 'voters'), 0.0012195121951219512)\n",
      "(('of', 'the', 'successful'), 0.0012195121951219512)\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import KneserNeyProbDist\n",
    "knerser_ney = KneserNeyProbDist(fdist)\n",
    "sample_prob = sort_samples_by_prob(knerser_ney)\n",
    "print(\"top 20 highest probability:\")\n",
    "for t in sample_prob[:20]: print(t)\n",
    "print(\"top 10 lowest probability:\")\n",
    "for t in sample_prob[-11:]: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果可以观察到，经过Knerser Ney平滑之后，所有词组的概率都有了大幅度的提升。并且值得注意的是，对于出现频率高的词组，前20个词组中那些在句子边缘的词组已经被平滑掉了，现在是一些语义较强的词组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model in NLTK\n",
    "在nltk中有一个专门的lm模块用于语言模型，包括预处理，n-gram模型以及各种平滑方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a train text [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'], ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.'], ['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.'], ['It', 'recommended', 'that', 'Fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.'], ['The', 'grand', 'jury', 'commented', 'on', 'a', 'number', 'of', 'other', 'topics', ',', 'among', 'them', 'the', 'Atlanta', 'and', 'Fulton', 'County', 'purchasing', 'departments', 'which', 'it', 'said', '``', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'to', 'the', 'best', 'interest', 'of', 'both', 'governments', \"''\", '.'], ['Merger', 'proposed'], ['However', ',', 'the', 'jury', 'said', 'it', 'believes', '``', 'these', 'two', 'offices', 'should', 'be', 'combined', 'to', 'achieve', 'greater', 'efficiency', 'and', 'reduce', 'the', 'cost', 'of', 'administration', \"''\", '.'], ['The', 'City', 'Purchasing', 'Department', ',', 'the', 'jury', 'said', ',', '``', 'is', 'lacking', 'in', 'experienced', 'clerical', 'personnel', 'as', 'a', 'result', 'of', 'city', 'personnel', 'policies', \"''\", '.']]\n"
     ]
    }
   ],
   "source": [
    "# prepare data. Here we only use the first 100 sentences from brown corpora.\n",
    "train_text = corpus_sents[:100]\n",
    "print(\"a train text\", train_text[:10])\n",
    "n = 2 # bigrams\n",
    "\n",
    "# Transform the text into bigrams.\n",
    "from nltk.util import bigrams, everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten \n",
    "\n",
    "# padding one sentence for bigrams and extract bigrams\n",
    "pad_sent_bigrams = bigrams(pad_both_ends(train_text[0], n=n))\n",
    "# print(list(pad_sent_bigrams))\n",
    "\n",
    "# padding one sentence for bigrams and extract unigram, bigrams\n",
    "all_grams = everygrams(list(pad_both_ends(train_text[0], n=n)))\n",
    "# print(list(all_grams))\n",
    "\n",
    "# padding all sentences and combine them as a list\n",
    "pad_sents = flatten(pad_both_ends(sent, n=n) for sent in train_text[:2])\n",
    "# print(list(pad_sents))\n",
    "\n",
    "# All above can be simplify to one function\n",
    "# from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "# train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "# train_grams: list of everygrams from padded train_text \n",
    "# pad_words: flat stream of padded train_text\n",
    "\n",
    "\n",
    "# print(id(train_grams))\n",
    "# train_l = list(train_grams)\n",
    "# print(id(train_l), len(train_l))\n",
    "# print(id(train_grams))\n",
    "# train_m = list(train_grams)\n",
    "# print(id(train_m), len(train_m))\n",
    "# 112218747912\n",
    "# 112219322888 100\n",
    "# 112218747912\n",
    "# 112216366664 0\n",
    "# !!! Cannot convert generator\n",
    "\n",
    "# Because of the above reason, we need to reproduce train_grams and pad_words for every language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import bigrams\n",
    "def test_lm(model):\n",
    "    test_word = \"the\" # ('the', 128) \n",
    "    print(\"score of test word {} is: {}\".format(test_word, model.score(test_word)))\n",
    "    \n",
    "    test_word = \"chairman\" # ('chairman', 2), probability of word with different smooth methods\n",
    "    print(\"score of test word {} is: {}\".format(test_word, model.score(test_word)))\n",
    "    \n",
    "    test_tuple = (\"Fulton\", \"County\") # probability of word `States` given context `United`\n",
    "    print(\"score of {} given {} is: {}\".format(test_tuple[1], test_tuple[0], \n",
    "                                               model.score(test_tuple[1], [test_tuple[0]])))\n",
    "    \n",
    "    \n",
    "    # test words don't occur in training corpus\n",
    "    test_word = \"alliance\"\n",
    "    print(\"score of not exist word {} is: {}\".format(test_word, model.score(test_word)))\n",
    "          \n",
    "    test_tuple = (\"New\", \"Year\")\n",
    "    print(\"score of not exist {} given {} is: {}\".format(test_tuple[1], test_tuple[0], \n",
    "                                               model.score(test_tuple[1], [test_tuple[0]])))\n",
    "    \n",
    "    test_sent = corpus_sents[150] # probability of a sentence, i.e., the total probability \n",
    "                                  # of n-gram tuples from this sentence\n",
    "    print(\"test sentence is: \", test_sent)\n",
    "    not_exist_words = [word for word in test_sent if model.counts[word] == 0]\n",
    "    print(\"not_exist_words: \", not_exist_words)\n",
    "    test_tokens = bigrams(pad_both_ends(test_sent, 2))\n",
    "    print(\"entropy of test sentence is: \", model.entropy(test_tokens)) # cross-entropy\n",
    "    print(\"perplexity of test sentence is: \", model.perplexity(test_tokens))\n",
    "# 由于困惑度计算的时候除以两个词共同出现的次数，从而导致除0错误。？？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 863 items>\n",
      "score of test word the is: 0.05186385737439222\n",
      "score of test word chairman is: 0.0008103727714748784\n",
      "score of County given Fulton is: 0.42857142857142855\n",
      "score of not exist word alliance is: 0.0\n",
      "score of not exist Year given New is: 0\n",
      "test sentence is:  ['Opponents', 'generally', 'argued', 'that', 'the', 'ballot', \"couldn't\", 'give', 'enough', 'information', 'about', 'tax', 'proposals', 'for', 'the', 'voters', 'to', 'make', 'an', 'intelligent', 'choice', '.']\n",
      "not_exist_words:  ['Opponents', 'argued', \"couldn't\", 'enough', 'information', 'proposals', 'intelligent', 'choice']\n",
      "entropy of test sentence is:  inf\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-689d5b846861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6c244360b74c>\u001b[0m in \u001b[0;36mtest_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_both_ends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cross-entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# 由于困惑度计算的时候除以两个词共同出现的次数，从而导致除0错误。？？？？？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         return -1 * _mean(\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Return average (aka mean) for sequence of items.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Maximum Likelihood Model\n",
    "from nltk.lm import MLE\n",
    "mle = MLE(n) # 2 is the highest order\n",
    "mle.fit(train_grams, pad_words)\n",
    "print(mle.vocab)\n",
    "test_lm(mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大似然对于未出现在训练集中的词语的计数为0，从而导致了计算熵的时候对数的底数为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of test word the is: 0.0387271089762834\n",
      "score of test word chairman is: 0.0009006304413089162\n",
      "score of County given Fulton is: 0.00798175598631699\n",
      "score of not exist word alliance is: 0.0003002101471029721\n",
      "score of not exist Year given New is: 0.0011587485515643105\n",
      "test sentence is:  ['Opponents', 'generally', 'argued', 'that', 'the', 'ballot', \"couldn't\", 'give', 'enough', 'information', 'about', 'tax', 'proposals', 'for', 'the', 'voters', 'to', 'make', 'an', 'intelligent', 'choice', '.']\n",
      "not_exist_words:  ['Opponents', 'argued', \"couldn't\", 'enough', 'information', 'proposals', 'intelligent', 'choice']\n",
      "entropy of test sentence is:  9.180327718582245\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-dcbd61bce041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6c244360b74c>\u001b[0m in \u001b[0;36mtest_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_both_ends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cross-entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# 由于困惑度计算的时候除以两个词共同出现的次数，从而导致除0错误。？？？？？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         return -1 * _mean(\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Return average (aka mean) for sequence of items.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Laplace (add-one) Smooth Model\n",
    "from nltk.lm import Laplace\n",
    "lm = Laplace(n)\n",
    "lm.fit(train_grams, pad_words)\n",
    "test_lm(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of test word the is: 0.04132894631249499\n",
      "score of test word chairman is: 0.0008827541930824172\n",
      "score of County given Fulton is: 0.010207939508506616\n",
      "score of not exist word alliance is: 0.00024075114356793194\n",
      "score of not exist Year given New is: 0.0011587485515643105\n",
      "test sentence is:  ['Opponents', 'generally', 'argued', 'that', 'the', 'ballot', \"couldn't\", 'give', 'enough', 'information', 'about', 'tax', 'proposals', 'for', 'the', 'voters', 'to', 'make', 'an', 'intelligent', 'choice', '.']\n",
      "not_exist_words:  ['Opponents', 'argued', \"couldn't\", 'enough', 'information', 'proposals', 'intelligent', 'choice']\n",
      "entropy of test sentence is:  9.12161165507209\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-fbce4d8642aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLidstone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6c244360b74c>\u001b[0m in \u001b[0;36mtest_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_both_ends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cross-entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# 由于困惑度计算的时候除以两个词共同出现的次数，从而导致除0错误。？？？？？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         return -1 * _mean(\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Return average (aka mean) for sequence of items.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Lidstone Smooth Model\n",
    "from nltk.lm import Lidstone\n",
    "lm = Lidstone(order=n, gamma=0.75)\n",
    "lm.fit(train_grams, pad_words)\n",
    "test_lm(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of test word the is: 0.05186385737439222\n",
      "score of test word chairman is: 0.0008103727714748784\n",
      "score of County given Fulton is: 0.42732021301227135\n",
      "score of not exist word alliance is: 0.0\n",
      "score of not exist Year given New is: 0.0\n",
      "test sentence is:  ['Opponents', 'generally', 'argued', 'that', 'the', 'ballot', \"couldn't\", 'give', 'enough', 'information', 'about', 'tax', 'proposals', 'for', 'the', 'voters', 'to', 'make', 'an', 'intelligent', 'choice', '.']\n",
      "not_exist_words:  ['Opponents', 'argued', \"couldn't\", 'enough', 'information', 'proposals', 'intelligent', 'choice']\n",
      "entropy of test sentence is:  inf\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-fd61879aaf22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWittenBellInterpolated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6c244360b74c>\u001b[0m in \u001b[0;36mtest_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_both_ends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cross-entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity of test sentence is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# 由于困惑度计算的时候除以两个词共同出现的次数，从而导致除0错误。？？？？？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self, text_ngrams)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         return -1 * _mean(\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Return average (aka mean) for sequence of items.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Witten Bell Smooth Model\n",
    "from nltk.lm import WittenBellInterpolated\n",
    "lm = WittenBellInterpolated(n)\n",
    "lm.fit(train_grams, pad_words)\n",
    "test_lm(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of test word the is: 0.0011587485515643105\n",
      "score of test word chairman is: 0.0011587485515643105\n",
      "score of County given Fulton is: 0.42148650885614963\n",
      "score of not exist word alliance is: 0.0011587485515643105\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-2923723d2b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKneserNeyInterpolated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6c244360b74c>\u001b[0m in \u001b[0;36mtest_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtest_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"New\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     print(\"score of not exist {} given {} is: {}\".format(test_tuple[1], test_tuple[0], \n\u001b[0;32m---> 21\u001b[0;31m                                                model.score(test_tuple[1], [test_tuple[0]])))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtest_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# probability of a sentence, i.e., the total probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m         return self.unmasked_score(\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/models.py\u001b[0m in \u001b[0;36munmasked_score\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmasked_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/smoothing.py\u001b[0m in \u001b[0;36malpha_gamma\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malpha_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprefix_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/lm/smoothing.py\u001b[0m in \u001b[0;36malpha\u001b[0;34m(self, word, prefix_counts)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprefix_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_grams, pad_words = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Kneser-Ney Smooth Model\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "lm = KneserNeyInterpolated(n)\n",
    "lm.fit(train_grams, pad_words)\n",
    "test_lm(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Models         | the(128) | chairman(2) | Fulton County(6) | alliance(not exist) | New Year(not exist) | sentence(has not exist words) entropy | sentence(has not exist words) perplexity |\n",
    "| -------------- | -------- | ----------- | ---------------- | ------------------- | ------------------- | ------------------------------------- | ---------------------------------------- |\n",
    "| MLE            | 0.051863 | 0.000810    | 0.428571         | 0.0                 | 0                   | inf                                   | ZeroDivisionError                        |\n",
    "| Laplace        | 0.038727 | 0.000900    | 0.007981         | 0.000300            | 0.001158            | 9.180327                              | ZeroDivisionError                        |\n",
    "| Lidstone(0.75) | 0.041328 | 0.000882    | 0.010207         | 0.000240            | 0.0011587           | 9.121611                              | ZeroDivisionError                        |\n",
    "| Witten Bell    | 0.051863 | 0.000810    | 0.427320         | 0.0                 | 0.0                 | inf                                   | ZeroDivisionError                        |\n",
    "| Kneser Ney     | 0.001158 | 0.001158    | 0.421486         | 0.001158            | ZeroDivisionError   | ZeroDivisionError                     | ZeroDivisionError                        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述汇总结果可以观察到：\n",
    "+ 对于出现在训练集中，且次数较多的词语`the`，使用平滑方法之后其概率都有所下降。对于不同的平滑方法，Lidstone方法比Laolace方法下降幅度小，这是因为gamma参数值小，从而导致除数略小；Witten Bell方法没有下降；Kneserver Ney方法平滑程度最大，下降至0.001158。\n",
    "+ 对于出现在训练集中，且次数较小的词语（chairman），使用平滑方法之后其概率有所上升。\n",
    "+ 对于句子的entropy来说，由于句子中出现了频次为0的词语，导致MLE方法中entropy为inf，这是正常的。使用了Laplace和Lidstone方法之后，其entropy有值，并且Lidstone的entropy比Laplace小。Witten Bell平滑方法的entropy为什么也是inf，不是使用了平滑了吗？？？另外，Kneser Ney方法又为何出现了除0错误，其内部如何实现？？？\n",
    "+ 对于句子的perplexity来说，由于计数为0，根据perplexity的计算公式导致了除0错误？？？\n",
    "+ 对于Kneser Ney方法来说，不管是出现频次高（the），频次低（chairman）或者没有出现的单词（alliance），其对应score都相同，难道是nltk内部实现方法有问题？？\n",
    "\n",
    "另外，由于一些平滑方法使用了低阶语法的信息，所以我们在构建模型的时候需要输入everygrams???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他语言模型工具\n",
    "### SRILM\n",
    "[SRILM](http://www.speech.sri.com/projects/srilm/)由SRI实验室开发，用来构建和应用统计语言模型，主要用于语音识别，统计标注和切分，以及机器翻译。使用c++语言实现。语言模型使用的是ngram模型，包括最大似然及一些平滑方法（Good-Turing，Katz回退，插值Kneser-Ney）。\n",
    "\n",
    "[http://www.52nlp.cn/language-model-training-tools-srilm-details](http://www.52nlp.cn/language-model-training-tools-srilm-details)\n",
    "[https://www.cnblogs.com/welen/p/7593222.html](https://www.cnblogs.com/welen/p/7593222.html)\n",
    "\n",
    "### IRSLTM\n",
    "[IRSLTM](https://github.com/irstlm-team/irstlm)由Trento FBK-IRST实验室开发，主要用于在大规模数据上训练语言模型。IRSLTM通过采用训练子语言模型最终融合的方法，达到减少内存消耗，提高训练速度的目的：\n",
    "+ 在训练语料上统计带词频词汇表；\n",
    "+ 按照词频均衡的原则将词汇表划分为若干个子词汇表；\n",
    "+ 对各个子词汇表统计 n-gram,这些 n-gram 必须以词汇表中的词汇开头；\n",
    "+ 根据第四步的统计结果,建立多个子语言模型；\n",
    "+ 把所有的子语言模型融合成最终语言模型；\n",
    "\n",
    "[http://www.52nlp.cn/language-modeling-toolkit-irstlm-installation-and-trial-noting](http://www.52nlp.cn/language-modeling-toolkit-irstlm-installation-and-trial-noting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
